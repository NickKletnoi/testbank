name: Run Github Notebook on Databricks
on:
  workflow_dispatch:

jobs:
  Deploy_Code:
    name: Deploy to Databricks workspace
    uses: .github/workflows/reusable_databricks_code_deployment.yml@v1.0.0
    with:
      databricks_code_path: "Pipelines/Dev"
      service_code_file: "notebooks/test1.py"
    secrets:
      DATABRICKS_HOST_URL: https://dbc-68d2f873-1a1e.cloud.databricks.com/
      DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_DEV_TOKEN }}

  Run_Notebook:
    needs: ['Deploy_Code']
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v2
      - name: Trigger notebook in staging
        uses: databricks/run-notebook@v0
        with:
          databricks-host: https://dbc-68d2f873-1a1e.cloud.databricks.com/
          databricks-token: ${{ secrets.DATABRICKS_DEV_TOKEN }}
          local-notebook-path: notebooks/test1.py
          #libraries-json: >
          #  [
          #    { "pypi": "mlflow" }
          #  ]
          # The cluster JSON below is for AWS workspaces. On Azure and GCP, set
          # node_type_id to an appropriate node type, e.g. "Standard_D3_v2" for
          # Azure or "n1-highmem-4" for GCP
          new-cluster-json: >
            {
              "num_workers": 1,
              "spark_version": "11.3.x-scala2.12",
              "node_type_id": "i3.xlarge"
            }
          # Grant users in the "devops" group view permission on the
          # notebook results
          #access-control-list-json: >
          #  [
          #    {
          #      "group_name": "devops",
          #      "permission_level": "CAN_VIEW"
          #    }
          #  ]
          # https://github.com/databricks/run-notebook/#run-notebook-v0
