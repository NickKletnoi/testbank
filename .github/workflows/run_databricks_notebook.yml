name: Run Databricks Notebook

on:
  workflow_dispatch:

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v2
      - name: Trigger notebook in DEV
        uses: databricks/run-notebook@v0
        with:
          databricks-host: https://xxx-staging.cloud.databricks.com
          databricks-token: ${{ secrets.DATABRICKS_DEV_TOKEN }}
          local-notebook-path: notebooks/test1.py
          # The cluster JSON below is for AWS workspaces. On Azure and GCP, set
          # node_type_id to an appropriate node type, e.g. "Standard_D3_v2" for
          # Azure or "n1-highmem-4" for GCP
          new-cluster-json: >
            {
              "num_workers": 1,
              "spark_version": "11.3.x-scala2.12",
              "node_type_id": "i3.xlarge"
            }
          # Grant users in the "devops" group view permission on the
          # notebook results
          access-control-list-json: >
            [
              {
                "group_name": "devops",
                "permission_level": "CAN_VIEW"
              }
            ]
